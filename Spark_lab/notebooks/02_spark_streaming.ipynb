{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Spark Streaming with Kafka\n",
        "\n",
        "## ğŸ¯ **Learning Objectives:**\n",
        "- Master Spark Structured Streaming\n",
        "- Learn real-time data processing patterns\n",
        "- Practice Kafka integration with Spark\n",
        "- Understand streaming analytics concepts\n",
        "- Implement real-time aggregations\n",
        "\n",
        "## ğŸ“š **Key Concepts:**\n",
        "1. **Structured Streaming**: Real-time data processing framework\n",
        "2. **Kafka Integration**: Reading from Kafka topics\n",
        "3. **Stream Processing**: Continuous data transformation\n",
        "4. **Watermarking**: Handling late-arriving data\n",
        "5. **Checkpointing**: Ensuring fault tolerance\n",
        "\n",
        "## ğŸ—ï¸ **Architecture Overview:**\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Kafka Topic   â”‚â”€â”€â”€â–¶â”‚  Spark Streaming â”‚â”€â”€â”€â–¶â”‚   Real-time     â”‚\n",
        "â”‚   (Stock Data)  â”‚    â”‚     Engine       â”‚    â”‚   Analytics     â”‚\n",
        "â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚                        â”‚                        â”‚\n",
        "         â–¼                        â–¼                        â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ High-frequency  â”‚    â”‚ Stream Processingâ”‚    â”‚ Output Sinks    â”‚\n",
        "â”‚ Data Producer   â”‚    â”‚ â€¢ Aggregations   â”‚    â”‚ â€¢ Console       â”‚\n",
        "â”‚                 â”‚    â”‚ â€¢ Window Functionsâ”‚    â”‚ â€¢ Memory        â”‚\n",
        "â”‚                 â”‚    â”‚ â€¢ Watermarking   â”‚    â”‚ â€¢ File System   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## ğŸ“Š **Streaming Use Cases:**\n",
        "- **Real-time Analytics**: Live dashboards and metrics\n",
        "- **Alert Systems**: Immediate notifications and triggers\n",
        "- **Data Pipeline**: Continuous ETL processes\n",
        "- **Monitoring**: Real-time system health checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install pyspark findspark pandas numpy pyarrow kafka-python\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.streaming import StreamingQuery\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "print(\"âœ… Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session for Streaming\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkStreamingLab\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
        "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"ğŸš€ Spark Streaming Session initialized successfully!\")\n",
        "print(f\"ğŸ“Š Spark Version: {spark.version}\")\n",
        "print(f\"ğŸ”— Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"ğŸ“ Checkpoint Location: /tmp/checkpoint\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
